{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twaldburger/flood475-presenter/blob/master/geo475_flood_modelling_in_gee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import time\n",
        "\n",
        "## set some parameters, please update if required\n",
        "PROJECT_ID = 'ee-timwaldburger-flood475' # your GEE project id\n",
        "SAMPLE_SIZE = 100 # number of training locations per flood event and class\n",
        "\n",
        "## connect to GEE\n",
        "try:\n",
        "    ee.Initialize()\n",
        "except ee.EEException:\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize(project=PROJECT_ID)"
      ],
      "metadata": {
        "id": "munACIb96Vfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Data exploration and visualization"
      ],
      "metadata": {
        "id": "gl1rsslKSJ7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## define the datasets we want to use for our model\n",
        "globalFlood = ee.ImageCollection(\"GLOBAL_FLOOD_DB/MODIS_EVENTS/V1\")\n",
        "dem = ee.ImageCollection('COPERNICUS/DEM/GLO30') \\\n",
        "        .select('DEM')\n",
        "landcover = ee.ImageCollection('ESA/WorldCover/v200')\n",
        "hydro = ee.Image('MERIT/Hydro/v1_0_1')\n",
        "prec = ee.ImageCollection('ECMWF/ERA5_LAND/HOURLY') \\\n",
        "         .select('total_precipitation')\n",
        "runoffPotential = ee.Image('projects/sat-io/open-datasets/HiHydroSoilv2_0/Hydrologic_Soil_Group_250m') \\\n",
        "                    .remap([1, 2, 3, 4, 14, 24, 34], [1, 2, 3, 4, 1, 3, 4])"
      ],
      "metadata": {
        "id": "pBHxsikA6Vbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Task:** Get an overview on the datasets we use. What data do they provide? Who produced them? What is their spatial and temporal resolution? A good starting point is the [Earth Engine Data Catalog](https://developers.google.com/earth-engine/datasets).  \n",
        "**@presenter:** Ask for people to explain the datasets. Ask them to mention special characteristics (e.g. very low resolution) and explain where they see problems that should be kept in mind."
      ],
      "metadata": {
        "id": "xMhp6-quctsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## create an interactive map -> create a very basic one and ask the people to add and style all layers"
      ],
      "metadata": {
        "id": "TmwnW78Of3Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**@presenter:** Ask if someone wants to share their map and explain what functions they used to create it."
      ],
      "metadata": {
        "id": "BUj9DEb7fU4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Training dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "Yv7ybAUMSDmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pointQuery(fc:ee.FeatureCollection, img:ee.Image, prop:str) -> ee.FeatureCollection:\n",
        "  \"\"\"\n",
        "  Returns pixel values at locations using a feature collection and an image.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  fc: : ee.FeatureCollection\n",
        "    Collection of points at which to query the image.\n",
        "  img : ee.Image\n",
        "    The image to query.\n",
        "  prop : str\n",
        "    Name of new property to hold the query results.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  ee.FeatureCollection\n",
        "    Input FeatureCollection with lookup values added as new property.\n",
        "  \"\"\"\n",
        "  fc = img.reduceRegions(collection=fc, reducer=ee.Reducer.first())\n",
        "  return fc.map(lambda feat: feat.set(prop, feat.get('first')))\n",
        "\n",
        "\n",
        "def removeProperty(fc:ee.FeatureCollection, prop:str) -> ee.FeatureCollection:\n",
        "  \"\"\"\n",
        "  Removes a property by name from a feature collection.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  fc : ee.FeatureCollection\n",
        "    Collection from which to remove the property.\n",
        "  prop : str\n",
        "    Property to remove.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  ee.FeatureCollection\n",
        "    Input collection without the removed property.\n",
        "  \"\"\"\n",
        "  selectProperties = fc.propertyNames().filter(ee.Filter.neq('item', prop))\n",
        "  return fc.select(selectProperties)\n",
        "\n",
        "\n",
        "def createSample(img:ee.Image) -> ee.FeatureCollection:\n",
        "  \"\"\"\n",
        "  Samples training dataset for a single flood image.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  img : ee.Image\n",
        "    Input image.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  ee.FeatureCollection\n",
        "    Sampled and enriched training data.\n",
        "  \"\"\"\n",
        "\n",
        "  ## subtract permanent water bodies from flooded areas\n",
        "  permanent = img.select('jrc_perm_water')\n",
        "  water = img.select('flooded')\n",
        "  flooded = water.subtract(permanent).gt(0)\n",
        "\n",
        "  ## get the total and maximum precipitation over 14 days prior to the event end date\n",
        "  end = img.getNumber('system:time_end')\n",
        "  start = end.subtract(1209600000) # timestamp in milliseconds: 60 * 60 * 24 * 14 * 1000\n",
        "  precSum = prec.filter(ee.Filter.date(start, end)).sum()\n",
        "  precMax = prec.filter(ee.Filter.date(start, end)).max()\n",
        "\n",
        "  ## sample equal number of flooded and non-flooded points\n",
        "  sample = flooded.stratifiedSample(numPoints=SAMPLE_SIZE, classBand='flooded', geometries=True)\n",
        "\n",
        "  ## add image id in case we want to join the event metadata later\n",
        "  sample = sample.map(lambda x: x.set('eventId', img.get('system:index')))\n",
        "\n",
        "  ## enrich sample by running point lookups on multiple datasets\n",
        "  sample = pointQuery(sample, dem.mosaic(), 'demElevationAbs')\n",
        "  sample = pointQuery(sample, ee.Terrain.aspect(dem.mosaic()), 'demAspect')\n",
        "  sample = pointQuery(sample, ee.Terrain.slope(dem.mosaic()), 'demSlope')\n",
        "  sample = pointQuery(sample, landcover.first(), 'landcover')\n",
        "  sample = pointQuery(sample, hydro.select('upa'), 'upa')\n",
        "  sample = pointQuery(sample, runoffPotential, 'runoffPot')\n",
        "  sample = pointQuery(sample, precSum, 'precSum')\n",
        "  sample = pointQuery(sample, precMax, 'precMax')\n",
        "\n",
        "  ## remove first-property\n",
        "  sample = sample.map(lambda feat: removeProperty(feat, 'first'))\n",
        "\n",
        "  ## normalize elevation\n",
        "  elevationRange = dem.mosaic().reduceRegion(geometry=img.geometry(), reducer=ee.Reducer.minMax())\n",
        "  min = ee.Number(elevationRange.get('DEM_min'))\n",
        "  max = ee.Number(elevationRange.get('DEM_max'))\n",
        "  def normalizeElevation(feat:ee.Feature) -> ee.Feature:\n",
        "    return feat.set('demElevationNorm', (ee.Number(feat.get('demElevationAbs')).subtract(min)).divide(max.subtract(min)))\n",
        "  sample = sample.filter(ee.Filter.notNull(ee.List(['demElevationAbs']))).map(normalizeElevation)\n",
        "\n",
        "  return sample"
      ],
      "metadata": {
        "id": "U7fUtqOy6VTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Task:** Try to understand the code in the cell above. Why are we using *ee.Image.stratifiedSample* in line 70 instead of using the much faster *ee.Image.Sample* method? Why are we using the complicated GEE methods in lines 65-76 and 90-94 instead of simply using plain Python? What does *ee.FeatureCollection.map* do? Why do we not just write a simple for-loop instead? Why do we normalize the elevation values?  \n",
        "**@presenter:** Ask where people looked up the GEE functions. Show the documentation section in the [GEE Code editor](https://code.earthengine.google.com/) if not mentioned.  \n",
        "**@presenter:**\n",
        "- *stratifiedSample* samples the same number of locations within each class. Since our flood footprints mostly contain non-flooded pixels, using *stratifiedSample* is a convenient way to avoid oversampling non-flooded pixels. However, there might also be a risk of *stratifiedSample* oversampling certain flooded areas if the flood footprint is very small.\n",
        "- We want to execute all the code in GEE itself so we can benefit from its optimization and parallelisation. If we would use plain Python, we would need to fetch the info from GEE server into our Colab Runtime which would make the whole process extremely inefficient.\n",
        "- *map* iterates over a feature collection and applies an algorithm to each feature. This process is run in parallel on the GEE server. Using a Python for-loop instead would loose the parallel execution and also create all the problems mentioned in the last question.\n",
        "- We use a global flood dataset, but look at the individual events independently. We do not want to create a bias towards the general elevation of the region where an event took place and are therefore normalizing the elevation within each image (and therefore for each event)."
      ],
      "metadata": {
        "id": "qL-_naeIdr1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## create enriched sample and store the result as an asset\n",
        "properties = ['demAspect', 'demElevationAbs', 'demElevationNorm', 'demSlope', 'eventId', 'flooded', 'landcover', 'precMax', 'precSum', 'runoffPot', 'upa', '.geo']\n",
        "sample = globalFlood.map(createSample).flatten()\n",
        "sample = sample.filter(ee.Filter.notNull(properties)).distinct(properties)\n",
        "task = ee.batch.Export.table.toAsset(sample, description='flood475-sampling', assetId=f\"projects/{PROJECT_ID}/assets/flood475_sample_2\")"
      ],
      "metadata": {
        "id": "W6ErNPvdBZm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## run and monitor the task\n",
        "task.start()\n",
        "while task.active():\n",
        "  ts = task.status()\n",
        "  if ts['start_timestamp_ms']>0:\n",
        "    s = round((ts['update_timestamp_ms']-ts['start_timestamp_ms'])/1000)\n",
        "  else:\n",
        "    s = round((ts['update_timestamp_ms']-ts['creation_timestamp_ms'])/1000)\n",
        "  print(f\"task '{ts['description']}' is {ts['state']} for {s} seconds\")\n",
        "  time.sleep(60)\n",
        "task.status()"
      ],
      "metadata": {
        "id": "s6I0aPqCXBIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Task:** There are multiple places (outside of Google Colab) where we can also monitor our tasks. Can you find them?  \n",
        "**@presenter:** Show the 3 places:\n",
        "- [GEE Code editor](https://code.earthengine.google.com/)\n",
        "- [Task Manager](https://code.earthengine.google.com/tasks)\n",
        "- [Tasks Page in the Cloud Console](https://console.cloud.google.com/earth-engine/tasks?project=ee-timwaldburger-flood475)"
      ],
      "metadata": {
        "id": "jvoojztSaYDJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87ySFUlm0bQj"
      },
      "source": [
        "# Creating a training dataset\n",
        "In this first notebook, we create a set of training locations which we can then use to train a flood prediction model.\n",
        "\n",
        "Please go through the explanations and code step-by-step and run each code cell.\n",
        "> **Task:** Tasks and questions are marked like this. Please try to answer them before proceeding with the next cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MH1oy-GBxDu"
      },
      "source": [
        "---\n",
        "## Preparations\n",
        "In this first section, we handle all imports and set some variables. We also initialize the connection to GEE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R9h73AZB23r"
      },
      "source": [
        "### Import dependencies\n",
        "All dependencies required for this notebook are pre-installed in Google Colab. We can therefore just import them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx4oC-Q069m5"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "import geemap\n",
        "import geemap.colormaps as cm\n",
        "import google\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxK350kN4lYj"
      },
      "source": [
        "### Define global variables\n",
        "The cell below defines some global variables. You are only required to set the project ID but can change the other variables if you want.\n",
        "- `PROJECT_ID` This is you Gee project ID. If you do not remember it, you can go to the [GEE code editor](https://code.earthengine.google.com/) and list your project by clicking on your user symbol in the top-right corner.\n",
        "- `SAMPLE_SIZE` This is the number of training locations we want to create. To keep computation times small, I suggest that you choose a sample size below 5000.\n",
        "- `TRAINING_DATA` This is the file name under which to save the training locations. Use different file names if you generate different training data. It is not required to add a file type.\n",
        "- `SEED` This is the seed value used for random sampling. Setting this here makes our sampling reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWMQwXpI4kxw"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = ''    # @param {type: 'string'}\n",
        "SAMPLE_SIZE = 3000 # @param {type: 'integer'}\n",
        "TRAINING_DATA = 'roi_sample' # @param {type: 'string'}\n",
        "SEED = 123         # @param {type: 'integer'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg5bGjW7HW-H"
      },
      "source": [
        "### Mount Google Drive\n",
        "We will use Google Drive to store our preliminary results from GEE because we can mount it to Google Colab and therefore easily write and read data without the need of manually down- and uploading datasets.\n",
        "\n",
        "**Important!** The cell below mounts your Google Drive to Google Colab and creates a new folder (named _geo475_ee_). This folder will be removed again at the end of the exercise (you can also keep it if you want, of course). **To make sure that we are not deleting any of your personal data, do not change the `data_dir`-variable in the cell below unless you know what you are doing.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6KMJWoGHXZb"
      },
      "outputs": [],
      "source": [
        "data_dir = Path('/content/gdrive/MyDrive/geo475_ee')\n",
        "\n",
        "## mount Google Drive to Colab\n",
        "if not data_dir.parent.exists():\n",
        "  google.colab.drive.mount('/content/gdrive')\n",
        "\n",
        "## create output directory for the project\n",
        "if not data_dir.exists():\n",
        "  data_dir.mkdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiHrA3i04P7j"
      },
      "source": [
        "### Initialize Google Earth Engine\n",
        "In the cell below, we connect to GEE using the same apporoach shown in [01_Connecting_to_GEE.ipynb](https://github.com/twaldburger/flood475/blob/master/01_Connecting_to_GEE.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUga2F8h4eAg"
      },
      "outputs": [],
      "source": [
        "google.colab.auth.authenticate_user()\n",
        "credentials, project_id = google.auth.default()\n",
        "ee.Initialize(credentials, project=PROJECT_ID)\n",
        "print(ee.String('Nice! That worked! :-)').getInfo())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXMLzsQtCYgM"
      },
      "source": [
        "---\n",
        "## Data\n",
        "In this section, we have a quick look at the datasets we will use to generate our training dataset. We use only data from the GEE Catalog for which you can find the links below:\n",
        "\n",
        "- [Global Flood Database v1 (2000-2018)](https://developers.google.com/earth-engine/datasets/catalog/GLOBAL_FLOOD_DB_MODIS_EVENTS_V1#description)\n",
        "- [CHIRPS Daily: Climate Hazards Group InfraRed Precipitation With Station Data (Version 2.0 Final)](https://developers.google.com/earth-engine/datasets/catalog/UCSB-CHG_CHIRPS_DAILY#description)\n",
        "- [Copernicus DEM GLO-30: Global 30m Digital Elevation Model](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_DEM_GLO30#description)\n",
        "- [MERIT Hydro: Global Hydrography Datasets](https://developers.google.com/earth-engine/datasets/catalog/MERIT_Hydro_v1_0_1#bands)\n",
        "- [ESA WorldCover 10m v200](https://developers.google.com/earth-engine/datasets/catalog/ESA_WorldCover_v200?hl=en#description)\n",
        "\n",
        "> **Task:** Familiarize yourself by following the links to the GEE Catalog. Try to answer the following questions:\n",
        "1. Which datasets are raster datasets? Which are vector?\n",
        "2. How do the data differ in (spatial) resolution?\n",
        "3. How many flood events does the [Global Flood Database v1 (2000-2018)](https://developers.google.com/earth-engine/datasets/catalog/GLOBAL_FLOOD_DB_MODIS_EVENTS_V1#description) contain?\n",
        "4. How many bands does the Digital Elevation Model (DEM) have?\n",
        "5. How is the precipitation dataset different from the others?\n",
        "6. How do these data limit us in training a flood prediction model? Try to look at the temporat and spatial extent covered by the different datasets. Hint: You can visualize a single day of CHIRPS precipitation data by running the next two code cells.\n",
        "\n",
        "With the code below, we import the relevant datasets from GEE. We are not reading the actual data into our memory (it would never fit) but we reference the datasets as a variables so we can start using it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6ESDkffKss6"
      },
      "source": [
        "### Import from GEE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8PKn32K8Oi1"
      },
      "outputs": [],
      "source": [
        "flood_ds = ee.ImageCollection('GLOBAL_FLOOD_DB/MODIS_EVENTS/V1')\n",
        "elevation_ds = ee.ImageCollection('COPERNICUS/DEM/GLO30')\n",
        "landcover_ds = ee.ImageCollection(\"ESA/WorldCover/v200\")\n",
        "precipitation_ds = ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY')\n",
        "flowaccumulation = ee.Image(\"MERIT/Hydro/v1_0_1\").select('upa')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqX-slZ4FmiH"
      },
      "source": [
        "### Explore interactively\n",
        "This code generates an interactive map of the precipitation data we are using. Note that although the dataset contains daily precipitation of over 30 years, we are only visualizing a single day.\n",
        "> **Task:** Try to visualize other datasets as well. Hint: Many datasets define the code for a simple visualsation in the catalog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMAeqJXQ8FCr"
      },
      "outputs": [],
      "source": [
        "## add a temporal filter to the CHIRPS dataset and select the precipitation layer\n",
        "precipitation = precipitation_ds.filterDate('2018-05-01').select('precipitation')\n",
        "\n",
        "## define a color palette for nicer visualisation\n",
        "precipitation_vis = {\n",
        "    'min': 1,\n",
        "    'max': 17,\n",
        "    'palette': ['001137', '0aab1e', 'e7eb05', 'ff4a2d', 'e90000'],\n",
        "}\n",
        "\n",
        "## create and display an interactive map\n",
        "Map = geemap.Map()\n",
        "Map.add_layer(precipitation, precipitation_vis, 'Precipitation')\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoI4EKW1Pmbh"
      },
      "source": [
        "---\n",
        "## Defining the training locations\n",
        "In this section, we sample the locations we want to use for training our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQEyXWRuK8Ue"
      },
      "source": [
        "### Defining the region of interest\n",
        "First, we define our region of interest.\n",
        "> **Task:** Run the cell below to visualize all historic flood events in our dataset. Decide which area you want to use to train your model and draw a region of interest (ROI) using the _Draw a rectangle_-tool from the tool bar on the left. Keep in mind the spatial constraints of our other datasets when choosing the ROI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtKL4FeUE76-"
      },
      "outputs": [],
      "source": [
        "## subtract the permanent water bodies from the flooded areas\n",
        "def mapper(img):\n",
        "  flood = img.select('flooded')\n",
        "  perm = img.select('jrc_perm_water')\n",
        "  return flood.multiply(perm.eq(0))\n",
        "all_floods = flood_ds.map(mapper).sum()\n",
        "\n",
        "## color flooded areas in blue\n",
        "flood_vis = {\n",
        "    'min': 0,\n",
        "    'max': 7,\n",
        "    'palette': cm.palettes.Blues\n",
        "    }\n",
        "\n",
        "## create and display an interactive map\n",
        "Map = geemap.Map()\n",
        "Map.add_layer(all_floods.selfMask(), flood_vis, 'Historic floods')\n",
        "Map.add_colorbar(flood_vis, label=\"Number of floods\", layer_name=\"Historic floods\", font_size=9)\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2lSQyE3MNni"
      },
      "source": [
        "### Sampling random locations\n",
        "We use GEE to randomly sample locations within our ROI. The ROI is taken from the Map-object we defined in the cell above.\n",
        "> **Task:** What needs to be considered if we want to sample evenly distributed locations? Hint: it also mentioned in the documentation of the function we use: [ee.FeatureCollection.randomPoints](https://developers.google.com/earth-engine/apidocs/ee-featurecollection-randompoints).\n",
        "\n",
        "Run the cell below to randomly sample training locations. Note that we use the two variables `SAMPLE_SIZE` and `SEED` from the beginning of the notebook as input for the sampling function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxQeQR7U_MAd"
      },
      "outputs": [],
      "source": [
        "## get the region of interest drawn on the map\n",
        "roi = ee.FeatureCollection(Map.draw_features)\n",
        "\n",
        "## randomly sample points within the ROI\n",
        "roi_sample = ee.FeatureCollection.randomPoints(\n",
        "    region=roi.geometry(),\n",
        "    points=SAMPLE_SIZE,\n",
        "    seed=SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lNwFlEYTC3m"
      },
      "source": [
        "---\n",
        "## Data enrichment\n",
        "In this section, we will enrich our training locations uning several different datasets. The approach is always the same, namely performing a simple point lookup on each dataset using the locations we sampled above.\n",
        "As the point lookup will be repeated several times, we define a custom function which we then just can apply to each dataset.\n",
        "\n",
        "It is important to know that calling this function does not yet trigger any computations on GEE. This will only happen once we create and start a task for it (which we will do further down below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRP500WJ3xlB"
      },
      "outputs": [],
      "source": [
        "def singleband_point_query(fc, img, scale, prop):\n",
        "  \"\"\"\n",
        "  Run a point lookup on a single image.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  fc : ee.FeatureCollection\n",
        "    FeatureCollection of points for which to preform the lookup.\n",
        "  img : ee.Image\n",
        "    Image on which to perform the lookup.\n",
        "  scale : float\n",
        "    Spatial resolution of the image.\n",
        "  prop : str\n",
        "    Name of the property under which lookup values shall be added to the feature collection.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  ee.FeatureCollection\n",
        "    Input FeatureCollection with lookup values added as new property.\n",
        "  \"\"\"\n",
        "\n",
        "  ## extract the pixel values at each location\n",
        "  fc = img.reduceRegions(\n",
        "      collection=fc,\n",
        "      reducer=ee.Reducer.first(),\n",
        "      scale=scale\n",
        "  )\n",
        "\n",
        "  ## add the pixel values as a new feature property\n",
        "  def mapper(feature):\n",
        "    return feature.set(prop, feature.get('first'))\n",
        "  fc = fc.map(mapper)\n",
        "\n",
        "  return fc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2VUtGXbTe5k"
      },
      "source": [
        "### Elevation, slope and aspect\n",
        "We start by enriching our training locations with information from the DEM. We not only use the elevation above sea level but also slope and aspect which we derive from the DEM using GEE's powerful built-in functions. Note how we are making use of the function we defined in the cell above.\n",
        "\n",
        "Dataset: [Copernicus DEM GLO-30: Global 30m Digital Elevation Model](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_DEM_GLO30#description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j_p5N7zyNBk"
      },
      "outputs": [],
      "source": [
        "## elevation\n",
        "dem = elevation_ds.select('DEM')\n",
        "roi_sample = singleband_point_query(\n",
        "    fc=roi_sample,\n",
        "    img=dem.mosaic(),\n",
        "    scale=dem.first().projection().nominalScale().getInfo(),\n",
        "    prop='elevation'\n",
        ")\n",
        "\n",
        "## slope\n",
        "slope = ee.Terrain.slope(dem.mosaic())\n",
        "roi_sample = singleband_point_query(\n",
        "    fc=roi_sample,\n",
        "    img=slope,\n",
        "    scale=slope.projection().nominalScale().getInfo(),\n",
        "    prop='slope'\n",
        ")\n",
        "\n",
        "## aspect\n",
        "aspect = ee.Terrain.aspect(dem.mosaic())\n",
        "roi_sample = singleband_point_query(\n",
        "    fc=roi_sample,\n",
        "    img=aspect,\n",
        "    scale=aspect.projection().nominalScale().getInfo(),\n",
        "    prop='aspect'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLmXZm2aTihK"
      },
      "source": [
        "### Land cover\n",
        "In this cell, we lookup the different landcover classes.\n",
        "\n",
        "Dataset: [ESA WorldCover 10m v200](https://developers.google.com/earth-engine/datasets/catalog/ESA_WorldCover_v200?hl=en#description)\n",
        "\n",
        "> **Task:** How many different classes does the dataset contain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FXTCTeQotYs"
      },
      "outputs": [],
      "source": [
        "roi_sample = singleband_point_query(\n",
        "    fc=roi_sample,\n",
        "    img=landcover_ds.first(),\n",
        "    scale=landcover_ds.first().projection().nominalScale().getInfo(),\n",
        "    prop='landcover'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1E7DDNoDTry"
      },
      "source": [
        "### Flow accumulation\n",
        "Next, we are determining the upstream drainage area (flow accumulation area) for each of our sample locations. Again, we are just using our custom function for the lookup.\n",
        "\n",
        "Dataset: [MERIT Hydro: Global Hydrography Datasets](https://developers.google.com/earth-engine/datasets/catalog/MERIT_Hydro_v1_0_1#bands)\n",
        "\n",
        "> **Task:** Which unit should we expect for the upstream drainage area?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc54vThODWFJ"
      },
      "outputs": [],
      "source": [
        "roi_sample = singleband_point_query(\n",
        "    fc=roi_sample,\n",
        "    img=flowaccumulation,\n",
        "    scale=flowaccumulation.projection().nominalScale().getInfo(),\n",
        "    prop='upstream_drainage_area'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_Tx_7DyTU3A"
      },
      "source": [
        "### Precipitation\n",
        "The precipitation data is different from the other datasets as it does not represent a certain point in time but actually contains over 15'000 individual layers representing over 30 years of daily precipitation. To reduce the number of input variables for our model, we will aggregate the data and only take the maximum precipitation for each pixel and within each year between 2000 and 2018.\n",
        "\n",
        "Dataset: [CHIRPS Daily: Climate Hazards Group InfraRed Precipitation With Station Data (Version 2.0 Final)](https://developers.google.com/earth-engine/datasets/catalog/UCSB-CHG_CHIRPS_DAILY#description)\n",
        "\n",
        "> **Task:** Why do we not use the full dataset? Why do we limit ourselves to only the years 2000 - 2018?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSVTlGCEnPf0"
      },
      "outputs": [],
      "source": [
        "for year in range(2000, 2019):\n",
        "  daily_max = precipitation_ds.select('precipitation').filter(ee.Filter.date(f\"{year}-01-01\", f\"{year+1}-01-01\")).max()\n",
        "  roi_sample = singleband_point_query(\n",
        "      fc=roi_sample,\n",
        "      img=daily_max,\n",
        "      scale=daily_max.projection().nominalScale().getInfo(),\n",
        "      prop=f\"daily_max_precipitation_-{2018-year}\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ7hbVCYM76Y"
      },
      "source": [
        "### Historic flood\n",
        "The last information which is missing is the information if a location has been flooded in the past or not. This will be our target variable for the model. Again, we can just use are custom function to run the point lookup on the historic floods dataset.\n",
        "\n",
        "Dataset: [Global Flood Database v1 (2000-2018)](https://developers.google.com/earth-engine/datasets/catalog/GLOBAL_FLOOD_DB_MODIS_EVENTS_V1#description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VmfC2AuM21H"
      },
      "outputs": [],
      "source": [
        "roi_sample = singleband_point_query(\n",
        "    fc=roi_sample,\n",
        "    img=all_floods,\n",
        "    scale=flood_ds.select('flooded').first().projection().nominalScale().getInfo(),\n",
        "    prop='floods'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7m9QdNl9LEf"
      },
      "source": [
        "### Compute and export results\n",
        "As mentioned above, calling our custom point query-function has not triggered any computations but we merely defined the exact steps which need to be executed. We will now create a task to run all these steps on Google Earth Engine. The results will be stored on your Google Drive as a csv-table. This cell might run for a few minutes so we can use the time to remember (and appreciate) what is actually happening:\n",
        "1. run point lookup on the DEM,\n",
        "2. compute slope from the DEM and run point lookup,\n",
        "3. compute aspect from the DEM and run point lookup,\n",
        "4. run point lookup on the land cover dataset,\n",
        "5. run point lookup upstream drainage area dataset,\n",
        "6. aggregate the yearly maximum daily precipitation for two decades and run point lookup,\n",
        "7. run point lookup on the historic floods dataset,\n",
        "8. write the result to a .csv-file.\n",
        "\n",
        "The computation time is depending on the size of your region of interest but keeping in mind that some of the datasets we use have spatial resolutions of 30 (or even 10) meters, the speed of Google Earth Engine is pretty impressive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOVtH-Ty_7o4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "## remove output file if it already exists\n",
        "try:\n",
        "   (data_dir/f\"{TRAINING_DATA}.csv\").unlink()\n",
        "except FileNotFoundError:\n",
        "  pass\n",
        "\n",
        "## create an export task\n",
        "task = ee.batch.Export.table.toDrive(\n",
        "    collection=roi_sample,\n",
        "    description='data export to Google Drive',\n",
        "    folder=data_dir.name,\n",
        "    fileNamePrefix=TRAINING_DATA,\n",
        "    fileFormat='CSV'\n",
        "    )\n",
        "\n",
        "## run and monitor export task\n",
        "task.start()\n",
        "while task.active():\n",
        "  sleep(10)\n",
        "task.status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiQpDbBhM6EE"
      },
      "source": [
        "With the code below, we import the .csv file generated in GEE from Google Drive. We can now - finally - see the results of our data enrichment.\n",
        "\n",
        "If the code below returns an error, you need to give it a few minutes since it might take some time for GEE to write the file to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQDM7-jvK-Gy"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_dir/f\"{TRAINING_DATA}.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXX7B53pM8BJ"
      },
      "source": [
        "### Normalize elevation\n",
        "In this last post-processing step, we normalize the elevation values to a range between 0 and 1. For this, we get the minimum and maximum elevation within our ROI and apply the [formula for linear scaling](https://developers.google.com/machine-learning/data-prep/transform/normalization#scaling-to-a-range)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsxkWc2XOcnP"
      },
      "outputs": [],
      "source": [
        "## determine min/max elevation within roi\n",
        "dem_range = dem.mosaic().reduceRegion(\n",
        "    geometry=roi.geometry(),\n",
        "    reducer=ee.Reducer.minMax(),\n",
        "    scale=dem.first().projection().nominalScale().getInfo(),\n",
        "    bestEffort=True\n",
        ").getInfo()\n",
        "\n",
        "\n",
        "## normalize elevation\n",
        "df['elevation'] = (df['elevation'] - dem_range.get('DEM_min')) / (dem_range.get('DEM_max') - dem_range.get('DEM_min'))\n",
        "\n",
        "## drop locations where any attribute is NaN\n",
        "df = df[~df.isnull().any(axis=1)]\n",
        "\n",
        "## write results to csv\n",
        "df.to_csv(data_dir/f\"{TRAINING_DATA}_norm.csv\", index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSA6PsDaHfrV"
      },
      "source": [
        "---\n",
        "## Optional: Repeating the same steps but for all 913 historic events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvYgQ2JURTsK"
      },
      "source": [
        "### Defining the training locations\n",
        "We sample the same number of locations as before but now, we not only sample within a single ROI but within the geometry of each and every event in our historic event dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6j5xMMMH0js"
      },
      "outputs": [],
      "source": [
        "## sample the same number of sample points for each flood event\n",
        "event_sample_size = SAMPLE_SIZE // flood_ds.size().getInfo()\n",
        "\n",
        "## randomly sample points within each event geometry\n",
        "def mapper(feat):\n",
        "  return ee.FeatureCollection.randomPoints(\n",
        "      region=feat.geometry(),\n",
        "      points=event_sample_size,\n",
        "      seed=SEED\n",
        "  )\n",
        "flood_sample = flood_ds.map(mapper).flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntF65MA5R-wc"
      },
      "source": [
        "### Data enrichment\n",
        "Here, we can repeat exactly the same steps as before. The only difference is that we use a different set of training locations for the point lookups. Actually, it would be elegant to define not only the point lookup but the whole data enrichment pipeline as a single function but I deliberately left that out to keep the code more readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnYLVlWOMsXc"
      },
      "outputs": [],
      "source": [
        "## elevation\n",
        "flood_sample = singleband_point_query(\n",
        "    fc=flood_sample,\n",
        "    img=dem.mosaic(),\n",
        "    scale=dem.first().projection().nominalScale().getInfo(),\n",
        "    prop='elevation'\n",
        ")\n",
        "\n",
        "## slope\n",
        "flood_sample= singleband_point_query(\n",
        "    fc=flood_sample,\n",
        "    img=slope,\n",
        "    scale=slope.projection().nominalScale().getInfo(),\n",
        "    prop='slope'\n",
        ")\n",
        "\n",
        "## aspect\n",
        "flood_sample = singleband_point_query(\n",
        "    fc=flood_sample,\n",
        "    img=aspect,\n",
        "    scale=aspect.projection().nominalScale().getInfo(),\n",
        "    prop='aspect'\n",
        ")\n",
        "\n",
        "## land cover\n",
        "flood_sample = singleband_point_query(\n",
        "    fc=flood_sample,\n",
        "    img=landcover_ds.first(),\n",
        "    scale=landcover_ds.first().projection().nominalScale().getInfo(),\n",
        "    prop='landcover'\n",
        ")\n",
        "\n",
        "## flow accumulation\n",
        "flood_sample = singleband_point_query(\n",
        "    fc=flood_sample,\n",
        "    img=flowaccumulation,\n",
        "    scale=flowaccumulation.projection().nominalScale().getInfo(),\n",
        "    prop='upstream_drainage_area'\n",
        ")\n",
        "\n",
        "## precipitation\n",
        "for year in range(2000, 2019):\n",
        "  daily_max = precipitation_ds.select('precipitation').filter(ee.Filter.date(f\"{year}-01-01\", f\"{year+1}-01-01\")).max()\n",
        "  flood_sample = singleband_point_query(\n",
        "      fc=flood_sample,\n",
        "      img=daily_max,\n",
        "      scale=daily_max.projection().nominalScale().getInfo(),\n",
        "      prop=f\"daily_max_precipitation_-{2018-year}\"\n",
        "  )\n",
        "\n",
        "## historic flood\n",
        "flood_sample = singleband_point_query(\n",
        "    fc=flood_sample,\n",
        "    img=all_floods,\n",
        "    scale=flood_ds.select('flooded').first().projection().nominalScale().getInfo(),\n",
        "    prop='floods'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paXlSWdkTV-k"
      },
      "source": [
        "Same as above, we have to explicitly create a task to actually run the computations on GEE. This will take a bit longer as the last one but while you wait, you can have a look at some apps built with GEE:\n",
        "- https://www.earthengine.app/\n",
        "- https://philippgaertner.github.io/2020/12/ee-apps-table-searchable/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ncUFRkBMsUS"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "## remove output file if it already exists\n",
        "try:\n",
        "   (data_dir/'flood_sample.csv').unlink()\n",
        "except FileNotFoundError:\n",
        "  pass\n",
        "\n",
        "## create an export task\n",
        "task = ee.batch.Export.table.toDrive(\n",
        "    collection=flood_sample,\n",
        "    description=\"data export to Google Drive\",\n",
        "    folder=data_dir.name,\n",
        "    fileNamePrefix='flood_sample',\n",
        "    fileFormat='CSV'\n",
        "    )\n",
        "\n",
        "## run and monitor export task\n",
        "task.start()\n",
        "while task.active():\n",
        "  sleep(10)\n",
        "task.status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9bfiFO4Uw1a"
      },
      "source": [
        "The normalization of elevation is a bit trickier than before because we do not want to normalize the full dataset but for each event separately.\n",
        "\n",
        "First, we want to exract all event geometries as individual polygons. This seems to be a rather hard task in GEE so we use [geopandas](https://geopandas.org/en/stable/) - a core library when working with vector data in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dP6eLXXZHnhD"
      },
      "outputs": [],
      "source": [
        "## get all image geometries as multipolygon\n",
        "multipolygon_fc = ee.FeatureCollection(flood_ds.select('flooded').geometry())\n",
        "\n",
        "## explode multipolygon to a list of polygons using geopandas\n",
        "gdf = geemap.ee_to_geopandas(multipolygon_fc)\n",
        "gdf.set_crs(4326, inplace=True)\n",
        "\n",
        "## create gee featurecollection of singlepolygons\n",
        "singlepolygons_fc = geemap.geopandas_to_ee(gdf.explode())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUxmaQnBV6hX"
      },
      "source": [
        "Second, we use the dataset we created in the cell above to get the minimum and maximum elevation within each event geometry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vd0MsOgaxIwM"
      },
      "outputs": [],
      "source": [
        "## define some input variables\n",
        "dem = elevation_ds.select('DEM')\n",
        "dem_mosaic = dem.mosaic()\n",
        "scale = dem.first().projection().nominalScale().getInfo()\n",
        "\n",
        "## define the mapper\n",
        "def mapper(img):\n",
        "\n",
        "  ## get the minimum and maximum elevation within the event geometry\n",
        "  dem_range = dem_mosaic.reduceRegion(\n",
        "      geometry=img.geometry(),\n",
        "      reducer=ee.Reducer.minMax(),\n",
        "      scale=scale,\n",
        "      bestEffort=True\n",
        "  )\n",
        "\n",
        "  ## create a feature and assign the relevant properties\n",
        "  feat = ee.Feature(img.geometry())\n",
        "  feat = feat.set('id', img.get('id'))\n",
        "  feat = feat.set('system_index', img.get('system:index'))\n",
        "  feat = feat.set('elevation_min', dem_range.get('DEM_min'))\n",
        "  feat =feat.set('elevation_max', dem_range.get('DEM_max'))\n",
        "\n",
        "  return feat\n",
        "\n",
        "## map the mapper over all images in the historic flood dataset\n",
        "dem_ranges = ee.FeatureCollection(flood_ds.select('flooded').map(mapper))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXfDT9jWW9VA"
      },
      "source": [
        "And again, we need to actually run the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm4tqt0RNk4F"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "## remove output file if it already exists\n",
        "try:\n",
        "   (data_dir/'dem_ranges.csv').unlink()\n",
        "except FileNotFoundError:\n",
        "  pass\n",
        "\n",
        "## create an export task\n",
        "task = ee.batch.Export.table.toDrive(\n",
        "    collection=dem_ranges,\n",
        "    description=\"data export to Google Drive\",\n",
        "    folder=data_dir.name,\n",
        "    fileNamePrefix='dem_ranges',\n",
        "    fileFormat='CSV'\n",
        "    )\n",
        "\n",
        "## run and monitor export task\n",
        "task.start()\n",
        "while task.active():\n",
        "  sleep(10)\n",
        "task.status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw41EFbEZIkd"
      },
      "source": [
        "Finally, we are importing the enriched training data and the dataset of min/max elevation so we can bring it together in order to create our final training dataset.\n",
        "\n",
        "> **Task:** Why do we need to normalize the elevation values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5c-55raf3z_"
      },
      "outputs": [],
      "source": [
        "## import data\n",
        "df = pd.read_csv(data_dir/'flood_sample.csv')\n",
        "ele = pd.read_csv(data_dir/'dem_ranges.csv')\n",
        "\n",
        "## add minimum and maximum elevation to training dataset\n",
        "df['system:index'] = df['system:index'].apply(lambda x: x.rsplit('_', 1)[0])\n",
        "df = df.merge(ele[['system:index', 'elevation_min', 'elevation_max']], how='left', on='system:index')\n",
        "\n",
        "## normalize elevation\n",
        "df['elevation'] = (df['elevation'] - df['elevation'].min()) / (df['elevation'].max() - df['elevation'].min())\n",
        "\n",
        "## drop locations where any attribute is NaN\n",
        "df = df[~df.isnull().any(axis=1)]\n",
        "\n",
        "## write dataframe to csv\n",
        "df.to_csv(data_dir/'flood_sample_norm.csv', index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjDC_B90cXpd"
      },
      "source": [
        "### Exploring the training dataset\n",
        "> **Task:** Run the remaining two cells and have a look at their output. Try to answer the following questions:\n",
        "1.  Is our training dataset biased? Try to think about the spatial and temporal limitations we mentioned when looking at the datasets.\n",
        "2. How would you rate the distribution of our training data with respect to the *flooded*-attribute? Is it optimal for our purpose?\n",
        "3. Where would you see room for improvement for the sampling approach? How would you implement your idea if you had two write code? No need to get too detailed but I am interested in your approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe5hJmPvcgnl"
      },
      "outputs": [],
      "source": [
        "## print flood counts\n",
        "for k,v in df['floods'].value_counts().to_dict().items():\n",
        "  print(f\"locations with {int(k)} floods: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rqH3Q1hc1ff"
      },
      "outputs": [],
      "source": [
        "## add sampled locations to the map of historic floods\n",
        "Map.add_layer(flood_sample, None, 'Training dataset')\n",
        "Map"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}